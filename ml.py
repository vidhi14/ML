# -*- coding: utf-8 -*-
"""ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lwk7V8o2gMRhCg1QGT5IZSUq4S2YR5nF

####**Supervised Learning** 
Supervised learning is a part of ML where we know the target values for a set of input values and figure out the realtionship between input and target values

> Supervised  learning problems can be classified as: 


**1. Regression:** Regression is a supervised learning algorithm where the output values are continuous(i.e. a continuous function maps input values to output values) 

**2. Classification :** Classification problems are those we get output as a from aset of values.

**Linear Regression: **when the target value depends only on one variable
"""

#hypothesis function
def hypothesis_function(o1,o2,input):
  hypothesis=[]
  for i in range(len(input)):
    value=o1+(o2*input[i]) 
    hypothesis.append(value) 
  return hypothesis

#Cost function 
def cost_function(target_values,hypothesis): 
  total_error=0 
  for in range(len(hypothesis)):
    value=(target_values[i]-hypothesis[i])**2
    total_error+=value
  error=total_error/len(hypothesis)
  return error

#Gradient Descent
def gradient_descent(o1,o2,input,target_values,hypothesis,learning_rate): 
  o1_new=0
  o2_new=0 
  for i in range(len(target_values)):
    o1_new+=2*o1*(hypothesis[i]-target_values[i])
    o2_new+=2*(hypothesis[i]-target_values[i]) 
  o1=(o1_new/len(input)*learning_rate 
  o2=(o2_new/len(input)*learning_rate 
  return o1,o2

#Training the model
def training(o1,o2,input,target_values,hypothesis,learning_rate,no_of_iterations): 
  cost_history=[]
  for i in range(no_of_iterations): 
    o1,o2=gradient_descent(o1,o2,inpt,tagtet_values,hypothesis,learning_rate) 
    cost=cost_function(target_values,hypothesis) 
    cost_history.append(cost) 
    if (i%10==0):
       print "iter={:d}    o2={:.2f}    o1={:.4f}    cost={:.2}".format(i, o2, o1, cost)
  return cost_history
